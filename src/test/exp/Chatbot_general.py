# -*- encoding: utf-8 -*-
"""
@Time    :   2024-01-25 01:05:35
@desc    :   simple chatbot
@Author  :   ticoAg
@Contact :   1627635056@qq.com
"""

import copy
import json
import os
import sys
from pathlib import Path

sys.path.append(Path("./").parent.as_posix())

import streamlit as st
from loguru import logger
from openai import OpenAI

from src.test.exp.data.prompts import Sysprompt

logger.remove()
logger.add(sink=sys.stderr, level="TRACE", backtrace=True, diagnose=True)
logger.remove()
logger.add(sink=sys.stderr, level="TRACE", backtrace=True, diagnose=True)
logger.add(
    Path("logs", "chatbot.log"),
    level="TRACE",
    encoding="utf-8",
    rotation="100 MB",
    retention="5 days",
    compression="gz",
    backtrace=True,
    diagnose=True,
)

client = OpenAI()
default_system_prompt = Sysprompt.system_prompt


class Args: ...


args = Args()


def dumpJS(obj):
    return json.dumps(obj, ensure_ascii=False)


def place_sidebar():
    with st.sidebar:
        client.base_url = st.text_input(
            "api base",
            key="openai_api_base",
            value=os.environ.get("OPENAI_API_BASE", ""),
        )
        api_key = st.text_input("api key", key="openai_api_key", value=None)
        client.api_key = api_key if api_key else os.environ.get("OPENAI_API_KEY", "")

        model_list = [i.id for i in client.models.list().data]
        args.model = st.selectbox("Choose your model", model_list, index=2)
        st.text_area(
            "system prompt",
            default_system_prompt,
            height=200,
            key="system_prompt",
            on_change=initlize_system_prompt,
        )
        prepare_parameters()
        # "[Get an OpenAI API key](https://platform.openai.com/account/api-keys)"
        "[View the source code](https://github.com/streamlit/llm-examples/blob/main/Chatbot.py)"
        # "[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/streamlit/llm-examples?quickstart=1)"


def prepare_parameters():
    """Initialize the parameters for the llm"""
    global args
    args.max_tokens = st.sidebar.slider("Max tokens", min_value=1, max_value=32000, value=4096, step=1)
    args.temperature = st.sidebar.slider(
        "Temperature",
        min_value=0.0,
        max_value=2.0,
        value=0.7,
        step=0.05,
        help="æ¨¡å‹åœ¨ç”Ÿæˆæ–‡æœ¬æ—¶çš„éšæœºæ€§ã€‚è¿™ä¸ªå‚æ•°çš„å€¼åœ¨0åˆ°2ä¹‹é—´ï¼Œè¶Šé«˜è¡¨ç¤ºæ¨¡å‹è¶Šå€¾å‘äºé€‰æ‹©ä¸å¤ªå¯èƒ½çš„å•è¯ï¼Œè¶Šä½è¡¨ç¤ºæ¨¡å‹è¶Šå€¾å‘äºé€‰æ‹©æœ€å¯èƒ½çš„å•è¯ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œè¿™ä¸ªå€¼è¶Šé«˜ï¼Œæ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬è¶Šæœ‰åˆ›æ„ï¼Œä½†ä¹Ÿå¯èƒ½ä¼šå‡ºç°æ›´å¤šçš„è¯­æ³•é”™è¯¯æˆ–ä¸åˆç†çš„å†…å®¹ã€‚",
    )
    args.top_p = st.sidebar.slider(
        "Top p",
        min_value=0.0,
        max_value=1.0,
        value=0.8,
        step=0.05,
        help="æ¨¡å‹åœ¨ç”Ÿæˆæ–‡æœ¬æ—¶çš„æ¦‚ç‡é˜ˆå€¼ã€‚è¿™ä¸ªå‚æ•°çš„å€¼åœ¨0åˆ°1ä¹‹é—´ï¼Œè¡¨ç¤ºæ¨¡å‹åªä¼šä»æ¦‚ç‡ä¹‹å’Œå¤§äºç­‰äºè¿™ä¸ªå€¼çš„å•è¯ä¸­é€‰æ‹©ä¸€ä¸ªã€‚è¿™ä¸ªå‚æ•°å¯ä»¥å¸®åŠ©æ¨¡å‹è¿‡æ»¤æ‰ä¸€äº›æç«¯çš„æˆ–ä¸ç›¸å…³çš„å•è¯ï¼Œæé«˜æ–‡æœ¬çš„è´¨é‡å’Œä¸€è‡´æ€§ã€‚",
    )
    # args.top_k = st.sidebar.slider(
    #     "Top k",
    #     min_value=-1,
    #     max_value=100,
    #     value=-1,
    #     step=1,
    #     help="æ¨¡å‹åœ¨ç”Ÿæˆæ–‡æœ¬æ—¶çš„å€™é€‰å•è¯çš„æ•°é‡ã€‚è¿™ä¸ªå‚æ•°çš„å€¼æ˜¯ä¸€ä¸ªæ•´æ•°ï¼Œè¡¨ç¤ºæ¨¡å‹åªä¼šä»æ¦‚ç‡æœ€é«˜çš„è¿™ä¹ˆå¤šä¸ªå•è¯ä¸­é€‰æ‹©ä¸€ä¸ªã€‚è¿™ä¸ªå‚æ•°å’Œtop_pç±»ä¼¼ï¼Œä¹Ÿå¯ä»¥å¸®åŠ©æ¨¡å‹è¿‡æ»¤æ‰ä¸€äº›ä¸åˆé€‚çš„å•è¯ï¼Œä½†æ˜¯å®ƒä¸è€ƒè™‘å•è¯çš„æ¦‚ç‡ï¼Œåªè€ƒè™‘æ’åã€‚è¿™ä¸ªå‚æ•°åœ¨æ‚¨çš„ä»£ç ä¸­è¢«æ³¨é‡Šæ‰äº†ï¼Œè¡¨ç¤ºä¸ä½¿ç”¨å®ƒã€‚",
    # )
    args.n = st.sidebar.slider(
        "N",
        min_value=1,
        max_value=50,
        value=1,
        step=1,
        help="æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬çš„æ•°é‡ã€‚è¿™ä¸ªå‚æ•°çš„å€¼æ˜¯ä¸€ä¸ªæ•´æ•°ï¼Œè¡¨ç¤ºæ¨¡å‹ä¼šæ ¹æ®åŒä¸€ä¸ªæç¤ºç”Ÿæˆå¤šå°‘ä¸ªä¸åŒçš„æ–‡æœ¬ã€‚è¿™ä¸ªå‚æ•°å¯ä»¥å¸®åŠ©æ‚¨æ¯”è¾ƒæ¨¡å‹çš„å¤šæ ·æ€§å’Œç¨³å®šæ€§ï¼Œæˆ–è€…ä»å¤šä¸ªé€‰é¡¹ä¸­é€‰æ‹©æœ€åˆé€‚çš„ä¸€ä¸ªã€‚",
    )
    args.presence_penalty = st.sidebar.slider(
        "Presence penalty",
        min_value=0.0,
        max_value=2.0,
        value=0.0,
        step=0.1,
        help="æ­¤å‚æ•°ç”¨äºé˜»æ­¢æ¨¡å‹åœ¨ç”Ÿæˆçš„æ–‡æœ¬ä¸­è¿‡äºé¢‘ç¹åœ°é‡å¤ç›¸åŒçš„å•è¯æˆ–çŸ­è¯­ã€‚å®ƒæ˜¯æ¯æ¬¡åœ¨ç”Ÿæˆçš„æ–‡æœ¬ä¸­å‡ºç°æ—¶éƒ½ä¼šæ·»åŠ åˆ°æ ‡è®°çš„å¯¹æ•°æ¦‚ç‡ä¸­çš„å€¼ã€‚è¾ƒé«˜çš„Frequency_penaltyå€¼å°†å¯¼è‡´æ¨¡å‹åœ¨ä½¿ç”¨é‡å¤æ ‡è®°æ—¶æ›´åŠ ä¿å®ˆã€‚",
    )
    args.frequency_penalty = st.sidebar.slider(
        "Frequency penalty",
        min_value=0.0,
        max_value=2.0,
        value=0.5,
        step=0.1,
        help="ç”¨æ¥æ§åˆ¶æ¨¡å‹ç”Ÿæˆæ–‡æœ¬æ—¶é¿å…é‡å¤ç›¸åŒçš„å•è¯æˆ–çŸ­è¯­ã€‚å®ƒæ˜¯ä¸€ä¸ªå€¼ï¼Œæ¯æ¬¡ç”Ÿæˆçš„å•è¯å‡ºç°åœ¨æ–‡æœ¬ä¸­æ—¶ï¼Œå°±ä¼šåŠ åˆ°è¯¥å•è¯çš„å¯¹æ•°æ¦‚ç‡ä¸Šã€‚è¿™ä¸ªå€¼è¶Šé«˜ï¼ˆæ¥è¿‘1ï¼‰ï¼Œæ¨¡å‹å°±è¶Šä¸å€¾å‘äºé‡å¤å•è¯æˆ–çŸ­è¯­ï¼›è¿™ä¸ªå€¼è¶Šä½ï¼ˆæ¥è¿‘0ï¼‰ï¼Œæ¨¡å‹å°±è¶Šå…è®¸é‡å¤ã€‚æ‚¨å¯ä»¥æ ¹æ®æ‚¨çš„éœ€æ±‚å’ŒæœŸæœ›çš„è¾“å‡ºæ¥è°ƒæ•´è¿™ä¸ªå‚æ•°çš„å€¼",
    )
    args.repetition_penalty = st.sidebar.slider(
        "Repetition penalty",
        min_value=0.0,
        max_value=1.0,
        value=1.0,
        step=0.1,
        help="ç”¨æ¥æ§åˆ¶æ¨¡å‹ç”Ÿæˆæ–‡æœ¬æ—¶é¿å…é‡å¤ç›¸åŒçš„å•è¯æˆ–çŸ­è¯­ã€‚å®ƒæ˜¯ä¸€ä¸ªå€¼ï¼Œæ¯æ¬¡ç”Ÿæˆçš„å•è¯å‡ºç°åœ¨æ–‡æœ¬ä¸­æ—¶ï¼Œå°±ä¼šä¹˜ä»¥è¯¥å•è¯çš„å¯¹æ•°æ¦‚ç‡ä¸Šã€‚è¿™ä¸ªå€¼è¶Šé«˜ï¼ˆæ¥è¿‘1ï¼‰ï¼Œæ¨¡å‹å°±è¶Šä¸å€¾å‘äºé‡å¤å•è¯æˆ–çŸ­è¯­ï¼›è¿™ä¸ªå€¼è¶Šä½ï¼ˆæ¥è¿‘0ï¼‰ï¼Œæ¨¡å‹å°±è¶Šå…è®¸é‡å¤ã€‚æ‚¨å¯ä»¥æ ¹æ®æ‚¨çš„éœ€æ±‚å’ŒæœŸæœ›çš„è¾“å‡ºæ¥è°ƒæ•´è¿™ä¸ªå‚æ•°çš„å€¼",
    )
    args.stop = ['\nObservation']


def initlize_system_prompt():
    """Initialize the system prompt"""
    st.session_state.messages = []
    st.session_state.messages.append({"role": "system", "content": st.session_state.system_prompt})

    # logger.debug(f"Update system_prompt:\n{st.session_state.system_prompt}")


def parse_response(text):
    # text = """Thought: æˆ‘å¯¹é—®é¢˜çš„å›å¤\nDoctor: è¿™é‡Œæ˜¯åŒ»ç”Ÿçš„é—®é¢˜æˆ–è€…ç»™å‡ºæœ€ç»ˆçš„ç»“è®º"""
    try:
        thought_index = text.find("Thought:")
        doctor_index = text.find("\nDoctor:")
        if thought_index == -1 or doctor_index == -1:
            return "None", text
        thought = text[thought_index + 8 : doctor_index].strip()
        doctor = text[doctor_index + 8 :].strip()
        return thought, doctor
    except Exception as err:
        logger.error(text)
        return "None", text


place_sidebar()


st.title("ğŸ’¬ Chatbot")
st.caption("ğŸš€ A streamlit chatbot powered by OpenSource LLM")

if "messages" not in st.session_state:
    st.session_state["messages"] = []

    if st.session_state.system_prompt:
        st.session_state.messages.append({"role": "system", "content": st.session_state.system_prompt})


for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("Your message"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        logger.debug(f"{dumpJS(args.__dict__)}")
        logger.debug(f"Input: \n{prompt}")
        for response in client.chat.completions.create(
            **args.__dict__, messages=st.session_state.messages, stream=True
        ):
            if not response.choices[0].delta.content:
                continue
            full_response += response.choices[0].delta.content
            message_placeholder.markdown(full_response + "â–Œ")
        logger.debug(f"Full response:\n{full_response}")
        message_placeholder.markdown(full_response)
    st.session_state.messages.append({"role": "assistant", "content": full_response})
    messages = copy.deepcopy(st.session_state.messages)
    logger.debug(f"Messages: {dumpJS(messages)}")
    logger.info("".center(80, "="))


# pip install openai --upgrade
# streamlit run Chatbot.py --server.port
